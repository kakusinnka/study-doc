## [ネットワークACLとセキュリティグループの違い](https://repost.aws/questions/QUwiwFKAbaTeCZsfuI9rBNBA/%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AFacl%E3%81%A8%E3%82%BB%E3%82%AD%E3%83%A5%E3%83%AA%E3%83%86%E3%82%A3%E3%82%B0%E3%83%AB%E3%83%BC%E3%83%97%E3%81%AE%E9%81%95%E3%81%84)

## [加速您的云驱动的数字化业务转型](https://aws.amazon.com/cn/cloud-adoption-framework/)

## [AWS 云财务管理服务](https://aws.amazon.com/cn/aws-cost-management/)

## [Amazon EC2 定价](https://aws.amazon.com/cn/ec2/pricing/)

## [预留实例 (RI)](https://aws.amazon.com/cn/aws-cost-management/aws-cost-optimization/reserved-instances/?nc1=h_ls)

## [Amazon EC2 预留实例](https://aws.amazon.com/cn/ec2/pricing/reserved-instances/)

## [Savings Plans FAQ](https://aws.amazon.com/cn/savingsplans/faq/)

## [AWS Well-Architected 框架](https://docs.aws.amazon.com/zh_cn/wellarchitected/latest/framework/the-pillars-of-the-framework.html)

## [什么是人工智能](https://aws.amazon.com/what-is/artificial-intelligence/)

## IAM（Identity and Access Management） 和 Access Policy（访问策略） 
### **3. IAM 和 Access Policy 的对比**

| **对比维度**     | **IAM（Identity and Access Management）** | **Access Policy（访问策略）**           |
| ---------------- | ----------------------------------------- | --------------------------------------- |
| **管理对象**     | 用户、组、角色、服务                      | AWS 资源（如 S3 存储桶、DynamoDB 表等） |
| **应用范围**     | 全局（可跨多个资源）                      | 资源级别（应用于特定资源）              |
| **策略附加位置** | 附加到用户、组、角色或服务                | 直接附加到资源                          |
| **跨账户访问**   | 通过角色实现                              | 通过策略直接允许跨账户访问              |
| **控制粒度**     | 更适合管理用户权限，支持细粒度控制        | 更适合管理资源权限，支持资源共享        |
| **典型使用场景** | 管理用户、组、角色的权限                  | 控制特定资源的访问权限                  |
| **策略格式**     | JSON 格式                                 | JSON 格式                               |
| **与资源的关系** | 间接管理资源（通过用户或角色访问资源）    | 直接管理资源                            |

---

### **总结**
- **IAM** 是以用户、组和角色为中心的权限管理工具，用于控制“谁可以做什么”。
- **Access Policy** 是以资源为中心的权限管理工具，用于控制“资源可以被谁访问”。

## 意图设置
意图设置（Intent Setting） 是自然语言处理（NLP）和对话系统（如聊天机器人、语音助手）中的一个关键概念。它主要用于理解用户输入的目的或意图，从而触发相应的操作或响应。在构建基于语言模型的系统时，意图设置是确保模型能够准确理解和响应用户需求的重要步骤。

## Intelligent Prompt Routing（インテリジェントプロンプトルーティング）
智能提示路由（Intelligent Prompt Routing）是一种基于提示（Prompt）的动态路由技术，用于在多个基础模型之间智能分配请求。它能够预测不同模型对特定请求的响应质量，并将请求路由到最适合处理该请求的模型，从而优化响应效果并降低成本。

## 学习率
### **什么是学习率（Learning Rate）？**

学习率（Learning Rate）是机器学习和深度学习模型训练中一个非常重要的超参数，它决定了模型在每次迭代中更新参数（如权重和偏置）时的步伐大小。简单来说，学习率控制了模型“学习”的速度。

---

### **学习率的作用**
学习率的大小直接影响模型的训练过程和最终效果：
1. **学习率过大**：
   - 模型每次更新的步伐太大，可能跳过最佳解（最优点），导致训练不稳定甚至发散。
2. **学习率过小**：
   - 模型每次更新的步伐太小，训练过程会非常慢，可能需要很长时间才能接近最佳解，甚至陷入局部最优解。

---

### **生活中的类比：爬山找山顶**
可以将学习率类比为“爬山时的步伐大小”，目标是找到山顶（最优解）：

1. **学习率过大：步伐太大**
   - 假设你要爬一座山，步伐太大时，每次迈步可能会直接从山的一侧跳到另一侧，甚至跳出山的范围。这样不仅找不到山顶，还可能导致你迷失方向（训练发散）。
   
   **例子**：
   - 你想走到房间的门口，但每次迈步都超过了门的位置，结果不断来回穿过门，却始终无法停在门口。

2. **学习率过小：步伐太小**
   - 如果你的步伐太小，每次只能向前移动一点点，即使方向是正确的，也需要很长时间才能到达山顶（最优解）。有时甚至可能因为太慢而卡在某个位置（局部最优解），错过真正的山顶。

   **例子**：
   - 你想从家走到超市，但每次只迈一小步，虽然方向对了，但可能需要几个小时才能到达。

---

### **学习率调整的重要性**
在实际的机器学习训练中，选择合适的学习率非常关键。如果学习率设置不当，会导致以下问题：
- 学习率太大：模型无法收敛，训练失败。
- 学习率太小：训练效率低下，浪费时间和资源。

---

### **总结**
学习率是模型训练中至关重要的参数，它像“步伐大小”一样，决定了训练的速度和稳定性。合理选择学习率，或者设计动态学习率调整策略，可以帮助模型更快、更稳定地找到最佳解。

## 特征工程
特征工程（Feature Engineering）是机器学习中非常重要的一步，指的是从原始数据中提取、转换和创建对模型训练有用的特征，以提升模型的性能和预测能力。简单来说，特征工程的目标是将原始数据转化为更能帮助模型“理解”的形式。

## 部分依存图（Partial Dependence Plots，PDPs）
部分依存图（Partial Dependence Plots，简称 PDPs）是一种可视化技术，用于解释机器学习模型中某个或某些特定特征对模型预测结果的影响。PDP 能够帮助我们理解模型的行为，尤其是在复杂的非线性模型（如随机森林、梯度提升树）中，它是解释性机器学习的重要工具之一。

## BERT
BERT（Bidirectional Encoder Representations from Transformers）是由Google提出的一种用于自然语言处理（NLP）的预训练模型。它在多个NLP任务中取得了显著的效果提升。

## BERTScore
BERTScore 是一种基于深度学习模型 BERT（Bidirectional Encoder Representations from Transformers）的文本评估指标，主要应用于自然语言处理（NLP）任务中，用于 **评估生成文本与参考文本之间的语义相似性**。与传统的基于词匹配的评估方法（如 BLEU、ROUGE 等）不同，BERTScore 利用 BERT 的上下文嵌入（contextual embeddings）来捕捉文本的深层语义信息，从而更精确地衡量文本之间的语义相似性。

## ROUGE （Recall-Oriented Understudy for Gisting Evaluation）以回忆为导向的概要评估研究
ROUGE（Recall-Oriented Understudy for Gisting Evaluation）是一组专门用于评估自动文本摘要和生成文本质量的指标，主要通过 **比较生成文本与参考文本之间的词或短语的重叠程度** 来衡量生成文本的质量。ROUGE 指标广泛用于自然语言处理（NLP）领域，尤其是在文本摘要、机器翻译和文本生成任务中。

## BLEU（Bilingual Evaluation Understudy, 双语评估替补）
BLEU 是一种用于评估机器翻译质量的指标，主要通过 **计算生成翻译与参考翻译在词汇和短语上的重叠程度来衡量翻译的准确性**。它是机器翻译领域最早且最广泛使用的自动化评价指标之一。

## RMSE（Root Mean Square Error，均方根误差）
RMSE 是一种常用的误差评估指标，主要用于 **衡量预测值（模型输出）与真实值（观测值）之间的差距**。它通过计算预测误差的平方平均值再开平方，得出一个可以直接反映误差大小的数值。

## ARIMA（Auto-Regressive Integrated Moving Average, 自回归积分滑动平均模型）
ARIMA 是一种经典的 **时间序列分析和预测模型**，广泛应用于经济学、金融、气象、市场趋势等领域。它通过**结合自回归（AR）、差分（I）和移动平均（MA）**三种机制，能够对时间序列数据进行建模和预测。

## K-means 聚类算法
K-means 是一种经典的无监督学习算法，用于将数据划分为 K 个簇（clusters）。它通过最小化各数据点到其所属簇中心的距离平方和，来实现数据的聚类。K-means 广泛应用于数据挖掘、图像分割、客户分群等领域。

## WaveNet 音声認識と音声合成
WaveNet 是由 Google DeepMind 于 2016 年提出的一种生成模型，专门用于生成高质量的音频波形。它通过深度神经网络直接对音频波形进行建模，能够生成自然且逼真的语音、音乐以及其他音频信号。WaveNet 的问世在语音合成（TTS）和音频生成领域引发了巨大的技术突破。

## GPT（生成式预训练变换器）
GPT（生成式预训练变换器）是一种基于深度学习的自然语言处理（NLP）模型，由 OpenAI 开发。它通过生成式预训练和自回归结构，在文本生成、理解和处理方面表现出色，成为当前人工智能领域的核心技术之一。

## Transformer
Transformer 是一种基于注意力机制（Attention Mechanism）的深度学习模型架构，它彻底改变了自然语言处理（NLP）领域，成为现代语言模型（如 GPT、BERT 等）的基础。Transformer 以其高效的并行计算能力和强大的上下文建模能力，广泛应用于机器翻译、文本生成、语义理解等任务。

## LDA（潜在狄利克雷分配，Latent Dirichlet Allocation）
LDA（Latent Dirichlet Allocation，潜在狄利克雷分配）是一种生成式概率模型，主要用于 **从非结构化文本数据中提取隐藏的主题**。它是一种常见的主题模型（Topic Model），被广泛应用于自然语言处理（NLP）领域，用于分析文档集合中的主题分布。

LDA 由 David Blei 等人在 2003 年提出，其核心思想是认为文档是由多个主题混合生成的，而每个主题又由一组单词组成。通过 LDA，可以自动发现文档中的潜在主题，并对文档进行主题建模和分类。

## 
| **指标**    | **定义**                                   | **适用场景**                                             |
| ----------- | ------------------------------------------ | -------------------------------------------------------- |
| **准确率**  | 正确预测的样本占总样本的比例               | 样本类别均衡时，适合作为整体性能的评估指标               |
| **精确率**  | 预测为正类的样本中，实际为正类的比例       | 假正例（FP）代价较高时，如垃圾邮件检测、医疗诊断         |
| **召回率**  | 实际为正类的样本中，被正确预测为正类的比例 | 假负例（FN）代价较高时，如疾病筛查、安全系统             |
| **F1 分数** | 精确率和召回率的调和平均                   | 精确率和召回率同等重要时，尤其适用于类别不均衡的分类任务 |

## 相关矩阵
**相关矩阵**是一种矩阵表示方法，用于显示多个变量之间的两两相关性。它是统计学和数据分析中常用的工具，能够帮助我们快速了解变量之间的线性关系。

相关矩阵中的每个元素表示两个变量之间的相关系数（通常是皮尔逊相关系数），其值范围为 \[-1, 1\]：
- **1** 表示完全正相关（两个变量完全同步变化）。
- **0** 表示没有线性相关性。
- **-1** 表示完全负相关（一个变量增加时，另一个变量减少）。

## EC2 实例类型快速记忆
| 实例类型  | 名字含义             | 特性                 | 适用场景               | 联想记忆                  |
| --------- | -------------------- | -------------------- | ---------------------- | ------------------------- |
| **I8g**   | **I = IOPS**         | 存储优化，快速读写   | 数据库、高速缓存       | **I = Input/Output**      |
| **R8g**   | **R = RAM**          | 内存优化，大内存     | 内存数据库、大数据分析 | **R = RAM**（内存任务）   |
| **Hpc7g** | **HPC = 高性能计算** | 高性能计算，科学模拟 | 工程模拟、科学计算     | **HPC = 科学计算**        |
| **Trn1**  | **Trn = Training**   | 机器学习训练优化     | 深度学习训练（如 LLM） | **Trn = Train**（AI训练） |

## **温度（Temperature）**、**Top-k**、**Top-p** 和 **最大生成长度（Max Tokens）**
### **1. 温度（Temperature）**
#### **定义**
- 温度是一个控制生成文本**随机性**的参数。
- 它的取值范围通常在 \[0, 1\] 之间，但也可以大于 1。
- 数值越低，生成的文本越**确定性**；数值越高，生成的文本越**随机性**。

#### **工作原理**
- 温度会影响模型对概率分布的采样：
  - 当温度较低时，模型倾向于选择概率最高的词（更确定）。
  - 当温度较高时，模型会更倾向于探索概率较低的词（更随机）。

---

### **2. Top-k 采样**
#### **定义**
- Top-k 是一种限制采样范围的方法，它控制模型在生成下一个词时，只从**概率最高的 k 个词**中进行选择。
- 这里的 **k** 是一个整数，表示候选词的数量。

#### **工作原理**
- 模型会对所有候选词按概率排序，并只保留前 **k 个词**，然后从中随机采样。
- **k 的大小**决定了生成结果的多样性：
  - 较小的 k：生成结果更集中于高概率词（更确定）。
  - 较大的 k：生成结果更随机。

---

### **3. Top-p 采样（Nucleus Sampling）**
#### **定义**
- Top-p 是另一种限制采样范围的方法，它根据候选词的累计概率来决定采样范围。
- **p** 是一个介于 \[0, 1\] 的阈值，表示累计概率的总和。

#### **工作原理**
- 模型会从所有候选词中按概率排序，并从中选择累计概率达到 **p** 的最小集合（通常称为“核”）。
- 然后，模型只从这个集合中随机采样。

---

### **4. 最大生成长度（Max Tokens）**
#### **定义**
- 最大生成长度限制了模型生成的**最大词（Token）数量**。
- 一个 Token 可以是一个单词、子词或字符（具体取决于模型的分词方式）。

#### **工作原理**
- 模型在生成结果时，最多生成 **Max Tokens** 个词，超过限制时会强制停止。
- 这个参数主要用于控制生成结果的篇幅。

---

### **总结对比**

| 参数             | 定义                   | 控制目标               | 适用场景           |
| ---------------- | ---------------------- | ---------------------- | ------------------ |
| **温度**         | 控制生成的随机性       | 确定性 vs 随机性       | 创意 vs 精确任务   |
| **Top-k**        | 限制候选词的数量       | 高概率词 vs 更多选择   | 确定性 vs 多样性   |
| **Top-p**        | 限制候选词的累计概率   | 动态范围采样 vs 灵活性 | 灵活性 vs 输出质量 |
| **最大生成长度** | 限制生成文本的最大长度 | 控制篇幅               | 短文 vs 长文生成   |

---

## 基准数据集 ベンチマークデータセット benchmark
基准数据集是用于评估和比较机器学习（ML）模型性能的标准化数据集合。

## epoch（训练轮数）（エポック）
在机器学习中，epoch（训练轮数）是指对整个训练数据集进行一次完整的学习过程。

## プロンプトの構成要素
![プロンプトの構成要素](./images/prompt-format.png)

## Moderation API モデレーションAPI 
モデレーションAPI是用于自动检测和管理不当内容的工具，广泛应用于多个平台。

## 梯度
可以通过一个生活中的例子来类比梯度的概念：

### 山坡与梯度
想象你站在一个雾气弥漫的山坡上，目标是找到山谷的最低点。由于雾气很浓，你看不到远处，只能通过脚下的坡度来判断方向。

1. **梯度的方向**
   - 梯度就像坡度的方向。如果你想快速下山，就要沿着坡度最陡的方向走。这个方向就是梯度的方向。

2. **梯度的大小**
   - 梯度的大小表示坡度的陡峭程度。坡度越陡，梯度越大，意味着你可以更快地向下移动。

3. **梯度下降**
   - 每次你走一小步，沿着梯度的方向前进，就像在进行梯度下降。你会逐渐接近山谷的最低点。

4. **调整步伐**
   - 走得太快（步长太大）可能会错过最低点；走得太慢（步长太小）可能会让你花费太多时间。因此，选择合适的步伐很重要，这类似于选择合适的学习率。

通过这个例子，梯度可以被理解为一种指引方向和速度的工具，帮助我们在复杂的地形中找到最优路径。

## 决定系数（R²）
决定系数（R²）是统计学和数据分析中用于评估回归模型性能的重要指标。它反映了模型对数据的拟合程度，或者说模型解释因变量变化的能力。

## 残差网络（ResNet, Residual Network）
残差网络（ResNet）通过引入残差连接解决了深度网络训练中的梯度消失和退化问题，使得训练非常深的网络成为可能。它的设计简单而高效，推动了深度学习的发展，在图像分类、目标检测、语义分割等任务中取得了卓越的表现。

## 检索增强生成（RAG, Retrieval-Augmented Generation）
检索增强生成（RAG）是一种结合信息检索和生成模型的技术，旨在提高生成式任务（如文本生成、问答系统）的性能。RAG 将传统的检索方法与现代生成模型相结合，使得生成的内容更加丰富和准确。

## Few-Shot Prompting
Few-Shot Prompting 的核心思想是在给大型语言模型的输入文本（Prompt）中，包含少量（"Few-Shot"）针对特定任务的输入-输出示例对。通过这些示例，模型能够理解你想要它完成的任务的模式、格式或要求，然后根据你最终提供的输入，生成符合该模式的输出。

## TensorFlow
一个由 Google 开发的流行的开源机器学习框架，用于构建和训练各种机器学习模型。

## PyTorch
一个由 Facebook 开发的开源机器学习框架，以其灵活性和动态计算图而闻名。

## Hugging Face
Hugging Face 本身是一个公司和社区，以其在自然语言处理 (NLP) 领域的贡献而闻名，特别是其流行的 transformers 库和 Hugging Face Hub。

transformers 库： 提供了大量预训练的、最先进的模型（如 BERT, GPT-2/3, T5, LLaMA 等），以及用于处理文本、音频、图像等的工具。这个库支持 PyTorch、TensorFlow 和 JAX 等多种深度学习框架。  
Hugging Face Hub： 是一个集中的平台，用户可以发现、分享和使用模型、数据集、演示空间 (Spaces) 等资源。  
Hugging Face on AWS 指的是在 Amazon Web Services (AWS) 云平台上使用和部署 Hugging Face 的工具、库和模型。结合 Hugging Face 的强大模型和易用性，以及 AWS 的可扩展、高性能的基础设施和托管服务，可以更高效地进行机器学习的开发、训练和部署。